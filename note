
#########################################################################################
TO RUN SOURCE CODE

# Activate virtual env:
source .env/bin/activate

# To use torch
export PATH=$PATH:/home/phanhuy1502/torch/install/bin
(Linux)

export PATH=$PATH:/Users/phanhuy1502/torch/install/bin
(Mac OS)

# To not use GPU OPTION: -gpu -1


#########################################################################################
USAGE
https://github.com/jcjohnson/torch-rnn

# 1. Preprocess data

python scripts/preprocess.py \
  --input_txt data/my_data.txt \
  --output_h5 models/model_name/my_data.h5 \
  --output_json models/model_name/my_data.json


# 2. Train the model

th train.lua -input_h5 models/model_name/my_data.h5 -input_json models/model_name/my_data.json

th train.lua -input_h5 data/big.h5 -input_json data/big.json -model_type lstm -num_layers 3 -rnn_size 128 -checkpoint_name models/sherlock_holmes_3_128

# 3. Sample from the model
(included in filling gaps)
- Using sample.lua
- e.g. th sample.lua -checkpoint cv/checkpoint_100000.t7 -length 2000 -gpu -1

th sample.lua -checkpoint cv/checkpoint_100000.t7 -length 2 -gpu -1 -start_text 'Indeed i'



#########################################################################################
PROJECT STRUCTURE

/gap: filling gap function
- multigap.lua: fill in a sequence with multiple gaps

/collectData: script to get data for training

/data: training data. Each training set include a .txt file, a .json file and a .h5 file

/models: storing trained model

/accuracy: scripts to measure accuracy of models
- generateTest.lua: functions to randomly put gaps in sequence + function to generate report
- runTestGroup.lua: function to run test and generate report on a test group
- /generateTestGroup: scripts to break a big sequence into smaller one (sequence-type-dependent)

# Checkpoint:
.json: in readable format --> still not sure why we need this
.t7: for torch to load/store pre-trained model. Apparently, torch can read this file and create a model (see sample.lua, line 19,20)

# Model used in the project:
model = nn.LanguageModel(opt_clone):type(dtype)
(train.lua, line 97)
--> Checkout nn.LanguageModel


#########################################################################################
MODELS

- models/cv: big.txt
- models/cv0:


#########################################################################################
TO IMPROVE

1) optimization:
right now, in singlegap.lua, everytime a new char is obtained, we 'sample' the model again,
repeating the feedforward process.
Improve: incorporate the change into LanguageModel itself --> don't repeat the feedforward process

2) check and get percentage

current implementation: just get the file and choose random blank.
What to be added: a file to split into smaller part to create smaller test cases -> won't be


3) autogeneration of tests

#########################################################################################
SETUP ON MAC

1. install hdf5
brew tap homebrew/science
brew install hdf5

2. install python requirement
virtualenv .env
source env/bin/activate
pip install -r requirements.txt

3. install torch

4. install luarocks required

# Install most things using luarocks
luarocks install torch
luarocks install nn
luarocks install optim
luarocks install lua-cjson

# We need to install torch-hdf5 from GitHub
git clone https://github.com/deepmind/torch-hdf5
cd torch-hdf5
luarocks make hdf5-0-0.rockspec
