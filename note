
#########################################################################################
TO RUN SOURCE CODE

# Activate virtual env: 
source .env/bin/activate 

# To use torch
export PATH=$PATH:/home/phanhuy1502/torch/install/bin

# To not use GPU OPTION: -gpu -1


#########################################################################################
USAGE
https://github.com/jcjohnson/torch-rnn

# 1. Preprocess data

python scripts/preprocess.py \
  --input_txt data/my_data.txt \
  --output_h5 models/model_name/my_data.h5 \
  --output_json models/model_name/my_data.json


# 2. Train the model

th train.lua -input_h5 models/model_name/my_data.h5 -input_json models/model_name/my_data.json


# 3. Sample from the model
(included in filling gaps)
- Using sample.lua 
- e.g. th sample.lua -checkpoint cv/checkpoint_100000.t7 -length 2000 -gpu -1

th sample.lua -checkpoint cv/checkpoint_100000.t7 -length 2 -gpu -1 -start_text 'Indeed i'



#########################################################################################
PROJECT STRUCTURE

/gap: filling gap function

/collectData: script to get data for training

/data: training data. Each training set include a .txt file, a .json file and a .h5 file

/models: storing trained model

/accuracy: scripts to measure accuracy of models

# Checkpoint:
.json: in readable format --> still not sure why we need this
.t7: for torch to load/store pretrained model. Apparenly, torch can read this file and create a model (see sample.lua, line 19,20)

# Model used in the project:
model = nn.LanguageModel(opt_clone):type(dtype)
(train.lua, line 97)
--> Checkout nn.LanguageModel
 

#########################################################################################
MODELS 

- models/cv: big.txt
- models/cv0: 


#########################################################################################
TO IMPROVE

1) optimization:
right now, in singlegap.lua, everytime a new char is obtained, we 'sample' the model again,
repeating the feedforward process.
Improve: incorporate the change into LanguageModel itself --> don't repeat the feedforward process