
#########################################################################################
TO RUN SOURCE CODE

# Activate virtual env:
source .env/bin/activate

# To use torch
export PATH=$PATH:/home/phanhuy1502/torch/install/bin
(Linux)

export PATH=$PATH:/Users/phanhuy1502/torch/install/bin
(Mac OS)

# To not use GPU OPTION: -gpu -1


#########################################################################################
USAGE
https://github.com/jcjohnson/torch-rnn

# 1. Preprocess data

python scripts/preprocess.py \
  --input_txt data/my_data.txt \
  --output_h5 models/model_name/my_data.h5 \
  --output_json models/model_name/my_data.json

python scripts/preprocess.py --input_txt data/sherlock_holmes_cleaned.txt --output_h5 data/sherlock_holmes_cleaned.h5 --output_json data/sherlock_holmes_cleaned.json

python scripts/preprocess.py --input_txt data/Vietnamese_GoneWithTheWind.txt --output_h5 data/Vietnamese_GoneWithTheWind.h5 --output_json data/Vietnamese_GoneWithTheWind.json

python scripts/preprocess.py --input_txt data/python_code_cleaned.txt --output_h5 data/python_code_cleaned.h5 --output_json data/python_code_cleaned.json

python scripts/preprocess.py --input_txt data/indonesian_cleaned.txt --output_h5 data/indonesian_cleaned.h5 --output_json data/indonesian_cleaned.json

python scripts/preprocess.py --input_txt data/sherlock_holmes_alpha4.txt --output_h5 data/sherlock_holmes_alpha4.h5 --output_json data/sherlock_holmes_alpha4.json

python scripts/preprocess.py --input_txt data/sherlock_holmes_alpha5.txt --output_h5 data/sherlock_holmes_alpha5.h5 --output_json data/sherlock_holmes_alpha5.json

# 2. Train the model

th train.lua -input_h5 models/model_name/my_data.h5 -input_json models/model_name/my_data.json

th train.lua -input_h5 data/big.h5 -input_json data/big.json -model_type lstm -num_layers 3 -rnn_size 128 -checkpoint_name models/sherlock_holmes_3_128

th train.lua -input_h5 data/big.h5 -input_json data/big.json -model_type lstm -num_layers 3 -rnn_size 128 -checkpoint_name models/sherlock_holmes_3_128

th train.lua -input_h5 data/sherlock_holmes_cleaned.h5 -input_json data/sherlock_holmes_cleaned.json -model_type lstm -num_layers 3 -rnn_size 128 -checkpoint_name models/sherlock_holmes_cleaned_3_128/sherlock_holmes_cleaned_3_128 -max_epochs 5

th train.lua -input_h5 data/Vietnamese_GoneWithTheWind.h5 -input_json data/Vietnamese_GoneWithTheWind.json -model_type lstm -num_layers 1 -rnn_size 128 -checkpoint_name models/Vietnamese_GoneWithTheWind_1_128/Vietnamese_GoneWithTheWind_1_128 -max_epochs 1 -gpu -1

# 3. Sample from the model
(included in filling gaps)
- Using sample.lua
- e.g. th sample.lua -checkpoint cv/checkpoint_100000.t7 -length 2000 -gpu -1

th sample.lua -checkpoint cv/checkpoint_100000.t7 -length 2 -gpu -1 -start_text 'Indeed i'



#########################################################################################
PROJECT STRUCTURE

/gap: filling gap function
- multigap.lua: fill in a sequence with multiple gaps

/collectData: script to get data for training

/data: training data. Each training set include a .txt file, a .json file and a .h5 file

/models: storing trained model

/accuracy: scripts to measure accuracy of models
- generateTest.lua: functions to randomly put gaps in sequence + function to generate report
- runTestGroup.lua: function to run test and generate report on a test group
- /generateTestGroup: scripts to break a big sequence into smaller one (sequence-type-dependent)

# Checkpoint:
.json: in readable format --> still not sure why we need this
.t7: for torch to load/store pre-trained model. Apparently, torch can read this file and create a model (see sample.lua, line 19,20)

# Model used in the project:
model = nn.LanguageModel(opt_clone):type(dtype)
(train.lua, line 97)
--> Checkout nn.LanguageModel


#########################################################################################
MODELS

- models/cv: big.txt
- models/cv0:


#########################################################################################
TO IMPROVE

1) optimization:
right now, in singlegap.lua, everytime a new char is obtained, we 'sample' the model again,
repeating the feedforward process.
Improve: incorporate the change into LanguageModel itself --> don't repeat the feedforward process

2) check and get percentage

current implementation: just get the file and choose random blank.
What to be added: a file to split into smaller part to create smaller test cases -> won't be


3) autogeneration of tests

#########################################################################################
SETUP ON MAC

1. install hdf5
brew tap homebrew/science
brew install hdf5

2. install python requirement
virtualenv .env
source env/bin/activate
pip install -r requirements.txt

3. install torch

4. install luarocks required

# Install most things using luarocks
luarocks install torch
luarocks install nn
luarocks install optim
luarocks install lua-cjson

# We need to install torch-hdf5 from GitHub
git clone https://github.com/deepmind/torch-hdf5
cd torch-hdf5
luarocks make hdf5-0-0.rockspec

# GPU
# install CUDA
